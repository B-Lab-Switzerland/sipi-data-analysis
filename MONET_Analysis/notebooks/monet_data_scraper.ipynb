{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b778b8-7a3b-4838-b5d0-069e78bd853b",
   "metadata": {},
   "source": [
    "# MONET2030 - Data Scraping/ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec5a19-35ce-4493-a4f9-56895f587a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stdlib imports\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# 3rd party imports\n",
    "import pandas as pd\n",
    "\n",
    "# Local imports\n",
    "from pymonet import monet_etl as etl\n",
    "from pymonet import monet_consts as const"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ca24e-68e4-4d96-b985-8d9ea24017a4",
   "metadata": {},
   "source": [
    "## 1) List of all MONET2030 indicators\n",
    "\n",
    "First, let's scrape a list of all indicators and their meta information (e.g. the URLs pointing to the indicator-specific subpages). Let's write this info to a dataframe and store it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b488169-044a-401b-bc54-3b25e4e6bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_indicator_table(indicator_table_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes the indicator table from the WWW.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indicator_table_url : str\n",
    "        URL pointing to the indicator table\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the full list of \n",
    "        MONET2030 indicators\n",
    "    \"\"\"\n",
    "    print(\"Scraping...\")\n",
    "    # ETL process for Monet2030 indicator list\n",
    "    etl_mil = etl.ETL_MonetIndicatorList(url_all_monet2030_indicators)\n",
    "    await etl_mil.extract()\n",
    "    etl_mil.transform()\n",
    "    etl_mil.df.to_csv(indicator_table_path)\n",
    "    print(\"-> done!\")    \n",
    "    \n",
    "    return etl_ml.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74418e6c-c611-4940-b65b-744238157d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not const.indicator_table_path.exists():  # Only scrape if the data is not already on disk\n",
    "    monet_indicator_df = await scrape_indicator_table(const.url_all_monet2030_indicators)\n",
    "else:  # Otherwise, read data from disk\n",
    "    print(\"Reading from disk...\")\n",
    "    monet_indicator_df = pd.read_csv(const.indicator_table_path).set_index(\"ID\")\n",
    "    print(\"-> done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1a191-03f3-455e-8b37-99b84907e811",
   "metadata": {},
   "source": [
    "## 2) List of all data files for all MONET2030 indicators\n",
    "\n",
    "Given a list of all subpages related to the MONET2030 indicators (see Step 1), we can now go a step further and scrape each of these subpages. Doing so we can find yet a new set of URLs that point to the actual indicator-specific data files. It is the data in these files we are ultimately interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478b3d9-2906-49f2-a671-a14c813697f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not const.summary_table_path.exists():\n",
    "    df_list = []\n",
    "    counter = 0\n",
    "    n_indicators = len(monet_indicator_df)\n",
    "    \n",
    "    start = dt.now()\n",
    "    print(\"Scraping...\")\n",
    "    for idx, indicator in monet_indicator_df.iterrows():\n",
    "        counter += 1\n",
    "        print(f\"{counter}/{n_indicators}\", end=\"\\r\")\n",
    "    \n",
    "        # ETL process for specific Monet2030 indicator\n",
    "        etl_mii = etl.ETL_MonetIndicatorInfo(indicator[\"Hyperlink\"])\n",
    "        await etl_mii.extract()\n",
    "        etl_mii.transform()\n",
    "    \n",
    "        # Augment data\n",
    "        etl_mii.df[\"Indicator\"] = indicator[\"Indicator\"]\n",
    "        etl_mii.df[\"SDG\"] = indicator[\"SDG\"]\n",
    "        etl_mii.df[\"Topic\"] = indicator[\"Topic\"]\n",
    "        df_list.append(etl_mii.df)\n",
    "    end = dt.now()\n",
    "    elapsed = end - start\n",
    "    print(\"-> done!\")\n",
    "    print(f\"Finished after {elapsed.seconds} seconds.\")\n",
    "\n",
    "    # Concatenate individual dfs\n",
    "    summary_table = pd.concat(df_list, ignore_index=True)\n",
    "    # Resort columns\n",
    "    summary_table = summary_table[[\"SDG\", \"Topic\", \"Indicator\", \"Observable\", \"Description\", \"Units\", \"damid\", \"Data_url\"]]\n",
    "    # Write resulting table to file\n",
    "    summary_table.to_csv(const.summary_table_path, index=False)\n",
    "else:\n",
    "    print(\"Reading from disk...\")\n",
    "    summary_table = pd.read_csv(const.summary_table_path)\n",
    "    print(\"-> done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82234b78-374e-4b2b-993e-a82695c47295",
   "metadata": {},
   "source": [
    "## 3) Download all the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d269a-e49b-4595-aac2-401a2aa5ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = []\n",
    "for href in complete_data_df[\"Data_url\"]:\n",
    "    database.append(pd.read_excel(href, sheet_name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6c16d-136b-496e-bf19-edeea62fa2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(database)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
